

# LLM DS Assistant

# ğŸ§  Architecture

```
sql
                         +------------------------+
                         |    Tabular Dataset     |
                         +-----------+------------+
                                     |
              +----------------------|----------------------+
              |                                              |
     +--------v--------+                            +--------v--------+
     |   CAAFE Agent   |                            |   Tuning Agent   |
     |  (Feature LLM)  |                            |  (Search LLM)    |
     +--------+--------+                            +--------+--------+
              |                                              |
    +---------v----------+                       +-----------v-----------+
    | New Feature Code   |                       | Suggested Search Space |
    +---------+----------+                       +-----------+-----------+
              |                                              |
    +---------v----------+                       +-----------v-----------+
    | CV-based Evaluator |                       | HalvingRandomSearchCV |
    +---------+----------+                       +-----------+-----------+
              |                                              |
     +--------v--------+                            +--------v--------+
     | Final Feature   |                            | Best Configs    |
     | Set (Transform) |                            | + Score History |
     +-----------------+                            +-----------------+

```


# CAAFE: Context-Aware Automated Feature Engineering with LLMs

Inspired by the paper [here](https://arxiv.org/pdf/2305.03403).

> âš™ï¸ Automatically generate meaningful features using Large Language Models + statistical validation
> ğŸ“Š Built for tabular classification tasks (esp. binary classification)
> ğŸ§  Designed by data scientists, for data scientists

---

## ğŸš€ Overview

**CAAFE (Context-Aware Automated Feature Engineering)** is a Python module that uses **LLMs** (like OpenAI's GPT-4) to generate **semantically meaningful, statistically validated features** for structured (tabular) datasets.

It tightly integrates LLM-driven code generation with fold-based evaluation to ensure that only performance-improving features are retained. The system is safe, extensible, and ideal for datasets where domain knowledge is hard to encode manually.

---

## âœ¨ Key Features

- ğŸ” **Iterative Feature Engineering**: Multiple LLM-driven iterations with early stopping.
- ğŸ“ˆ **Cross-Validated Evaluation**: Accept features only if they improve metrics (AUC or Accuracy).
- ğŸ§  **Contextual Prompting**: LLM has access to dataset summaries, feature stats, mutual information, and correlation tools.
- ğŸ” **Safe Code Validation**: All generated Python code is validated using AST parsing and a custom whitelist.
- ğŸ“œ **Explainable & Reproducible**: Each feature is saved with its reasoning and can be exported to `.py` or `.md`.
- ğŸ§° **Compatible with sklearn Pipelines**: Provided as a drop-in `sklearn.TransformerMixin`.



| Area            | Pattern Used                            | Notes                               |
| --------------- | --------------------------------------- | ----------------------------------- |
| Validation      | Pydantic + AST check                    | Ensures safe, interpretable code    |
| Modularity      | Sklearn Transformer, Tools, Agent       | Each component is loosely coupled   |
| Logging         | Structured + Human-readable             | Supports debugging, reproducibility |
| Evaluation      | CV-based wrapper                        | Metric-agnostic, robust             |
| LLM integration | Tool-augmented, prompt-reflective       | Avoids static prompts               |
| Persistence     | `.save_code()`and `.load_code_path` | Enables reuse in deployment         |

---

```
python
from src import CAAFETransformer
from sklearn.ensemble import RandomForestClassifier

# Assume df contains your tabular dataset and 'target' is your target column
X = df.drop(columns="target")
y = df["target"]

transformer = CAAFETransformer(
    target_name="target",
    dataset_description="Binary classification of customer churn",
    optimization_metric="auc",
    base_classifier=RandomForestClassifier(),
    llm_model="gpt-4",
    iterations=5
)

# Run the iterative feature engineering
transformer.fit(X, y)

# Apply to training or test data
X_enhanced = transformer.transform(X)

# Save the generated features for reuse
transformer.save_code("generated_features.md") # or as .py
```

```

```

### ğŸ§ª 2. LLM-Based Hyperparameter Tuning

**XGBoostTuner** is a smart AutoML component that guides the search for optimal model settings using LLMs and search history.

* ğŸ“Š Uses `HalvingRandomSearchCV` to efficiently search space
* ğŸ”„ Automatically refines the search space over multiple rounds
* ğŸ§  LLM explains each suggestion using dataset characteristics and best practices
* ğŸ” Tracks best configurations, iterations, and score progression
* ğŸ“ˆ Supports any sklearn scoring metric (e.g., `f1`, `roc_auc`, `precision`)


```
python
from src import XGBoostTuner

tuner = XGBoostTuner(dataset=df, target="target")
tuner.tune(max_iterations=5)

best_config = tuner.get_best_config()
print("Best XGBoost config:", best_config)
```

```

```
